{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1533e0f0",
      "metadata": {},
      "source": [
        "# Wi-Fi AP Clustering (k-means vs. Deterministic Annealing)\n",
        "\n",
        "This notebook mirrors `clustering.py`: it clusters 287 access points from the\n",
        "`data 3 access point` dataset using two optimizers on the same k-means SSE objective.\n",
        "- Baseline: multi-start k-means++ + Lloyd across k=2..6 (keep the best silhouette, enforce min cluster size).\n",
        "- Optimizer: deterministic annealing (k=3) with temperature-based soft\u2192hard assignments.\n",
        "\n",
        "Run the cells top-to-bottom. Outputs: metrics, cluster summaries, and PCA plots saved to\n",
        "`baseline_clusters.png` and `annealing_clusters.png`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c1bd6957",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452fce13",
      "metadata": {},
      "source": [
        "## Data loading and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a7139b2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "DATASET_DIR = Path(\"dataset\") / \"data 3 access point\"\n",
        "\n",
        "\n",
        "def load_raw_metrics() -> Dict[str, pd.DataFrame]:\n",
        "    paths = {\n",
        "        \"client\": DATASET_DIR / \"client_metrics_uap_5min.csv.gz\",\n",
        "        \"cpu\": DATASET_DIR / \"cpu_metrics_uap_5min.csv.gz\",\n",
        "        \"mem\": DATASET_DIR / \"memory_metrics_uap_5min.csv.gz\",\n",
        "        \"sig24\": DATASET_DIR / \"signal_24g_metrics_uap_5min.csv.gz\",\n",
        "        \"sig5\": DATASET_DIR / \"signal_5g_metrics_uap_5min.csv.gz\",\n",
        "    }\n",
        "    return {name: pd.read_csv(path) for name, path in paths.items()}\n",
        "\n",
        "\n",
        "def aggregate_features(dfs: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
        "    client_agg = dfs[\"client\"].groupby(\"ap_name\")[\"client_count\"].agg([\"mean\", \"max\", \"std\"])\n",
        "    client_agg[\"std\"] = client_agg[\"std\"].fillna(0)\n",
        "\n",
        "    cpu_agg = dfs[\"cpu\"].groupby(\"ap_name\")[\"cpu_usage_ratio\"].agg([\"mean\", \"max\"])\n",
        "    mem_agg = dfs[\"mem\"].groupby(\"ap_name\")[\"memory_usage_ratio\"].agg([\"mean\", \"max\"])\n",
        "    sig24_agg = dfs[\"sig24\"].groupby(\"ap_name\")[\"signal_dbm\"].agg([\"mean\", \"min\", \"max\"])\n",
        "    sig5_agg = dfs[\"sig5\"].groupby(\"ap_name\")[\"signal_dbm\"].agg([\"mean\", \"min\", \"max\"])\n",
        "\n",
        "    features = pd.concat(\n",
        "        [\n",
        "            client_agg.add_prefix(\"clients_\"),\n",
        "            cpu_agg.add_prefix(\"cpu_\"),\n",
        "            mem_agg.add_prefix(\"mem_\"),\n",
        "            sig24_agg.add_prefix(\"sig24_\"),\n",
        "            sig5_agg.add_prefix(\"sig5_\"),\n",
        "        ],\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    # Keep all APs by mean-imputing missing metrics per feature\n",
        "    features = features.apply(lambda col: col.fillna(col.mean()))\n",
        "    return features\n",
        "\n",
        "\n",
        "def standardize(values: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    mean = values.mean(axis=0)\n",
        "    std = values.std(axis=0)\n",
        "    return (values - mean) / std, mean, std\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "237f44e9",
      "metadata": {},
      "source": [
        "## Metrics helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a6a5cb3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sse_and_labels(data: np.ndarray, centroids: np.ndarray) -> Tuple[float, np.ndarray]:\n",
        "    d2 = ((data[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n",
        "    labels = d2.argmin(axis=1)\n",
        "    sse = float(d2[np.arange(len(data)), labels].sum())\n",
        "    return sse, labels\n",
        "\n",
        "\n",
        "def silhouette_values(data: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
        "    unique = np.unique(labels)\n",
        "    if len(unique) == 1:\n",
        "        return np.zeros(len(data))\n",
        "    pairwise = np.linalg.norm(data[:, None, :] - data[None, :, :], axis=2)\n",
        "    silhouettes = np.zeros(len(data))\n",
        "    for i in range(len(data)):\n",
        "        same = labels == labels[i]\n",
        "        if same.sum() > 1:\n",
        "            a = pairwise[i, same].sum() / (same.sum() - 1)\n",
        "        else:\n",
        "            a = 0.0\n",
        "        b = min(pairwise[i, labels == c].mean() for c in unique if c != labels[i])\n",
        "        silhouettes[i] = (b - a) / max(a, b) if max(a, b) > 0 else 0.0\n",
        "    return silhouettes\n",
        "\n",
        "\n",
        "def manual_silhouette(data: np.ndarray, labels: np.ndarray) -> float:\n",
        "    return float(silhouette_values(data, labels).mean())\n",
        "\n",
        "\n",
        "def davies_bouldin_index(data: np.ndarray, labels: np.ndarray) -> float:\n",
        "    unique = np.unique(labels)\n",
        "    k = len(unique)\n",
        "    if k < 2:\n",
        "        return float('nan')\n",
        "    centroids = np.array([data[labels == c].mean(axis=0) for c in unique])\n",
        "    scatter = np.array([\n",
        "        np.mean(np.linalg.norm(data[labels == c] - centroids[i], axis=1)) if (labels == c).any() else 0.0\n",
        "        for i, c in enumerate(unique)\n",
        "    ])\n",
        "    dbi = 0.0\n",
        "    for i in range(k):\n",
        "        ratios = []\n",
        "        for j in range(k):\n",
        "            if i == j:\n",
        "                continue\n",
        "            dist = np.linalg.norm(centroids[i] - centroids[j])\n",
        "            ratios.append((scatter[i] + scatter[j]) / max(dist, 1e-12))\n",
        "        dbi += max(ratios)\n",
        "    return float(dbi / k)\n",
        "\n",
        "\n",
        "def calinski_harabasz_index(data: np.ndarray, labels: np.ndarray) -> float:\n",
        "    unique = np.unique(labels)\n",
        "    k = len(unique)\n",
        "    n = len(data)\n",
        "    if k < 2 or n == k:\n",
        "        return float('nan')\n",
        "    overall_mean = data.mean(axis=0)\n",
        "    centroids = np.array([data[labels == c].mean(axis=0) for c in unique])\n",
        "    sizes = np.array([(labels == c).sum() for c in unique])\n",
        "    between = sum(sizes[i] * np.linalg.norm(centroids[i] - overall_mean) ** 2 for i in range(k))\n",
        "    within = sum(np.sum((data[labels == c] - centroids[i]) ** 2) for i, c in enumerate(unique))\n",
        "    return float((between / max(k - 1, 1e-12)) / (within / max(n - k, 1e-12)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b3f4cf",
      "metadata": {},
      "source": [
        "## Baseline: multi-start k-means++ + Lloyd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f73df9cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def kmeans_pp_init(data: np.ndarray, k: int, rng: np.random.Generator) -> np.ndarray:\n",
        "    n = len(data)\n",
        "    centroids = [data[rng.integers(0, n)]]\n",
        "    while len(centroids) < k:\n",
        "        d2 = np.min(((data[:, None, :] - np.array(centroids)[None, :, :]) ** 2).sum(axis=2), axis=1)\n",
        "        probs = d2 / d2.sum()\n",
        "        centroids.append(data[rng.choice(n, p=probs)])\n",
        "    return np.array(centroids)\n",
        "\n",
        "\n",
        "def lloyd_kmeans(\n",
        "    data: np.ndarray,\n",
        "    centroids: np.ndarray,\n",
        "    max_iter: int = 100,\n",
        "    tol: float = 1e-4,\n",
        "    rng: np.random.Generator | None = None,\n",
        ") -> Tuple[np.ndarray, np.ndarray, float]:\n",
        "    rng = rng or np.random.default_rng()\n",
        "    k = centroids.shape[0]\n",
        "    for _ in range(max_iter):\n",
        "        sse, labels = sse_and_labels(data, centroids)\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for j in range(k):\n",
        "            mask = labels == j\n",
        "            if mask.any():\n",
        "                new_centroids[j] = data[mask].mean(axis=0)\n",
        "            else:\n",
        "                new_centroids[j] = data[rng.integers(0, len(data))]\n",
        "        shift = np.linalg.norm(new_centroids - centroids)\n",
        "        centroids = new_centroids\n",
        "        if shift < tol:\n",
        "            break\n",
        "    sse, labels = sse_and_labels(data, centroids)\n",
        "    return centroids, labels, sse\n",
        "\n",
        "\n",
        "def multi_start_kmeans(\n",
        "    data: np.ndarray,\n",
        "    k_range: Iterable[int],\n",
        "    restarts: int = 100,\n",
        "    min_cluster_size: int = 5,\n",
        "    seed: int = 99,\n",
        ") -> Dict[str, object]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    best = None\n",
        "    for k in k_range:\n",
        "        for _ in range(restarts):\n",
        "            centroids = kmeans_pp_init(data, k, rng)\n",
        "            centroids, labels, sse = lloyd_kmeans(data, centroids, rng=rng)\n",
        "            sizes = np.bincount(labels, minlength=k)\n",
        "            if sizes.min() < min_cluster_size:\n",
        "                continue\n",
        "            sil = manual_silhouette(data, labels)\n",
        "            key = (sil, -sse)\n",
        "            if (best is None) or (key > best[\"key\"]):\n",
        "                best = {\n",
        "                    \"key\": key,\n",
        "                    \"k\": k,\n",
        "                    \"centroids\": centroids.copy(),\n",
        "                    \"labels\": labels.copy(),\n",
        "                    \"sizes\": sizes,\n",
        "                    \"sse\": sse,\n",
        "                    \"silhouette\": sil,\n",
        "                }\n",
        "    if best is None:\n",
        "        raise RuntimeError(\"No valid clustering found; try relaxing min_cluster_size or adjusting k_range.\")\n",
        "    return best\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c3b857",
      "metadata": {},
      "source": [
        "## Optimizer: deterministic annealing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5e2547fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "def deterministic_annealing_cluster(\n",
        "    data: np.ndarray,\n",
        "    k: int = 3,\n",
        "    T0: float = 6.0,\n",
        "    Tmin: float = 0.01,\n",
        "    alpha: float = 0.9,\n",
        "    inner_steps: int = 12,\n",
        "    seed: int = 123,\n",
        ") -> Dict[str, object]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n, _ = data.shape\n",
        "    centroids = data[rng.choice(n, k, replace=False)].copy()\n",
        "    best_centroids = centroids.copy()\n",
        "\n",
        "    def current_sse_labels(cents: np.ndarray) -> Tuple[float, np.ndarray]:\n",
        "        d2 = ((data[:, None, :] - cents[None, :, :]) ** 2).sum(axis=2)\n",
        "        labels = d2.argmin(axis=1)\n",
        "        sse = float(d2[np.arange(n), labels].sum())\n",
        "        return sse, labels\n",
        "\n",
        "    sse, labels = current_sse_labels(centroids)\n",
        "    best_sse = sse\n",
        "    best_labels = labels.copy()\n",
        "    history: List[Tuple[float, float]] = []\n",
        "\n",
        "    T = T0\n",
        "    while T > Tmin:\n",
        "        for _ in range(inner_steps):\n",
        "            d2 = ((data[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n",
        "            weights = np.exp(-d2 / max(T, 1e-8))\n",
        "            weights = weights / (weights.sum(axis=1, keepdims=True) + 1e-12)\n",
        "            denom = weights.sum(axis=0)[:, None] + 1e-12\n",
        "            centroids = (weights.T @ data) / denom\n",
        "        sse, labels = current_sse_labels(centroids)\n",
        "        history.append((T, sse))\n",
        "        if sse < best_sse:\n",
        "            best_sse = sse\n",
        "            best_centroids = centroids.copy()\n",
        "            best_labels = labels.copy()\n",
        "        T *= alpha\n",
        "\n",
        "    final_sse, final_labels = current_sse_labels(best_centroids)\n",
        "    final_silhouette = manual_silhouette(data, final_labels)\n",
        "    return {\n",
        "        \"centroids\": best_centroids,\n",
        "        \"labels\": final_labels,\n",
        "        \"sse\": final_sse,\n",
        "        \"silhouette\": final_silhouette,\n",
        "        \"sizes\": np.bincount(final_labels, minlength=k),\n",
        "        \"history\": history,\n",
        "        \"k\": k,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4a17929",
      "metadata": {},
      "source": [
        "## Reporting helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "46c54bd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_clusters(features: pd.DataFrame, labels: np.ndarray) -> List[Dict[str, object]]:\n",
        "    summaries = []\n",
        "    for c in np.unique(labels):\n",
        "        mask = labels == c\n",
        "        cluster_df = features.iloc[mask]\n",
        "        summaries.append(\n",
        "            {\n",
        "                'cluster': int(c),\n",
        "                'size': int(mask.sum()),\n",
        "                'clients_mean': cluster_df['clients_mean'].mean(),\n",
        "                'clients_max': cluster_df['clients_max'].mean(),\n",
        "                'cpu_mean': cluster_df['cpu_mean'].mean(),\n",
        "                'mem_mean': cluster_df['mem_mean'].mean(),\n",
        "                'sig24_mean': cluster_df['sig24_mean'].mean(),\n",
        "                'sig5_mean': cluster_df['sig5_mean'].mean(),\n",
        "            }\n",
        "        )\n",
        "    return sorted(summaries, key=lambda x: -x['size'])\n",
        "\n",
        "\n",
        "def silhouette_summary(values: np.ndarray) -> Dict[str, float]:\n",
        "    return {\n",
        "        'min': float(np.min(values)),\n",
        "        'q25': float(np.quantile(values, 0.25)),\n",
        "        'median': float(np.median(values)),\n",
        "        'q75': float(np.quantile(values, 0.75)),\n",
        "        'max': float(np.max(values)),\n",
        "        'mean': float(np.mean(values)),\n",
        "    }\n",
        "\n",
        "\n",
        "def save_results(results: Dict[str, object]) -> None:\n",
        "    with open('results_summary.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    lines = [\n",
        "        'Clustering results',\n",
        "        f\"Baseline MS k-means++ (k={results['baseline']['k']}): SSE={results['baseline']['sse']:.2f}, \"\n",
        "        f\"silhouette={results['baseline']['silhouette']:.3f}, sizes={results['baseline']['sizes']}, \"\n",
        "        f\"DBI={results['baseline']['dbi']:.3f}, CH={results['baseline']['calinski_harabasz']:.2f}\",\n",
        "        f\"Deterministic Annealing (k={results['annealing']['k']}): SSE={results['annealing']['sse']:.2f}, \"\n",
        "        f\"silhouette={results['annealing']['silhouette']:.3f}, sizes={results['annealing']['sizes']}, \"\n",
        "        f\"DBI={results['annealing']['dbi']:.3f}, CH={results['annealing']['calinski_harabasz']:.2f}\",\n",
        "        '',\n",
        "        'Silhouette summary (baseline):',\n",
        "        json.dumps(results['baseline']['silhouette_summary']),\n",
        "        'Silhouette summary (annealing):',\n",
        "        json.dumps(results['annealing']['silhouette_summary']),\n",
        "        '',\n",
        "        'k-scan (baseline):',\n",
        "        pd.DataFrame(results['baseline_scan']).to_csv(index=False),\n",
        "        'k-scan (annealing):',\n",
        "        pd.DataFrame(results['annealing_scan']).to_csv(index=False),\n",
        "        '',\n",
        "        'Baseline summary (per cluster):',\n",
        "        pd.DataFrame(results['baseline']['summary']).to_csv(index=False),\n",
        "        'Annealing summary (per cluster):',\n",
        "        pd.DataFrame(results['annealing']['summary']).to_csv(index=False),\n",
        "    ]\n",
        "    with open('results_summary.txt', 'w') as f:\n",
        "        f.write('\n",
        "'.join(lines))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f44fc5e4",
      "metadata": {},
      "source": [
        "## Visualization helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7646f0fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pca_project(data: np.ndarray, n_components: int = 2) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    mean = data.mean(axis=0)\n",
        "    centered = data - mean\n",
        "    _, _, vt = np.linalg.svd(centered, full_matrices=False)\n",
        "    components = vt[:n_components].T\n",
        "    projected = centered @ components\n",
        "    return projected, components, mean\n",
        "\n",
        "\n",
        "def project_points(points: np.ndarray, components: np.ndarray, mean: np.ndarray) -> np.ndarray:\n",
        "    return (points - mean) @ components\n",
        "\n",
        "\n",
        "def plot_clusters(coords: np.ndarray, labels: np.ndarray, centroids: np.ndarray, title: str, outfile: str) -> None:\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    palette = plt.cm.tab10.colors\n",
        "    unique = np.unique(labels)\n",
        "    for c in unique:\n",
        "        mask = labels == c\n",
        "        plt.scatter(\n",
        "            coords[mask, 0],\n",
        "            coords[mask, 1],\n",
        "            s=25,\n",
        "            color=palette[c % len(palette)],\n",
        "            alpha=0.7,\n",
        "            label=f\"Cluster {c} (n={mask.sum()})\",\n",
        "        )\n",
        "    plt.scatter(\n",
        "        centroids[:, 0],\n",
        "        centroids[:, 1],\n",
        "        s=140,\n",
        "        marker='X',\n",
        "        color='black',\n",
        "        label='Centroids',\n",
        "        edgecolor='white',\n",
        "        linewidth=1.0,\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outfile, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_silhouette_hist(baseline_vals: np.ndarray, anneal_vals: np.ndarray, outfile: str) -> None:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
        "    axes[0].hist(baseline_vals, bins=20, color='#3b82f6', alpha=0.8, edgecolor='white')\n",
        "    axes[0].set_title('Baseline silhouettes')\n",
        "    axes[0].set_xlabel('Silhouette')\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[1].hist(anneal_vals, bins=20, color='#10b981', alpha=0.8, edgecolor='white')\n",
        "    axes[1].set_title('Annealing silhouettes')\n",
        "    axes[1].set_xlabel('Silhouette')\n",
        "    fig.suptitle('Silhouette distribution by method', fontsize=12)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(outfile, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_cooling_curve(history: List[Tuple[float, float]], outfile: str) -> None:\n",
        "    if not history:\n",
        "        return\n",
        "    temps, sses = zip(*history)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(temps, sses, marker='o', color='#f59e0b')\n",
        "    plt.gca().invert_xaxis()\n",
        "    plt.xlabel('Temperature (T)')\n",
        "    plt.ylabel('SSE')\n",
        "    plt.title('Deterministic annealing cooling path')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outfile, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_k_scan(k_values: List[int], baseline_metrics: List[Dict[str, float]], anneal_metrics: List[Dict[str, float]], outfile: str) -> None:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    baseline_sse = [m['sse'] for m in baseline_metrics]\n",
        "    anneal_sse = [m['sse'] for m in anneal_metrics]\n",
        "    baseline_sil = [m['silhouette'] for m in baseline_metrics]\n",
        "    anneal_sil = [m['silhouette'] for m in anneal_metrics]\n",
        "    ax1 = plt.gca()\n",
        "    ax1.plot(k_values, baseline_sse, label='Baseline SSE', marker='o', color='#2563eb')\n",
        "    ax1.plot(k_values, anneal_sse, label='Annealing SSE', marker='o', color='#f97316')\n",
        "    ax1.set_xlabel('k')\n",
        "    ax1.set_ylabel('SSE')\n",
        "    ax1.tick_params(axis='y', labelcolor='black')\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(k_values, baseline_sil, label='Baseline silhouette', marker='s', linestyle='--', color='#1d4ed8')\n",
        "    ax2.plot(k_values, anneal_sil, label='Annealing silhouette', marker='s', linestyle='--', color='#ea580c')\n",
        "    ax2.set_ylabel('Silhouette')\n",
        "    ax2.tick_params(axis='y', labelcolor='black')\n",
        "    lines_labels = ax1.get_legend_handles_labels()\n",
        "    lines_labels2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines_labels[0] + lines_labels2[0], lines_labels[1] + lines_labels2[1], loc='best')\n",
        "    plt.title('Model quality vs. k')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outfile, dpi=200)\n",
        "    plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3adac3d",
      "metadata": {},
      "source": [
        "## Run pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e688ddcb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded features: 287 APs x 13 features\n",
            "Baseline MS k-means++ (k=3): SSE=2803.64, silhouette=0.212, sizes=[86, 25, 176]\n",
            "Deterministic annealing (k=3): SSE=2731.39, silhouette=0.166, sizes=[88, 82, 117]\n",
            "Saved PCA visualizations to baseline_clusters.png and annealing_clusters.png\n",
            "Wrote results_summary.json and results_summary.txt\n"
          ]
        }
      ],
      "source": [
        "# 1) Load and aggregate features\n",
        "dfs = load_raw_metrics()\n",
        "features = aggregate_features(dfs)\n",
        "print(f\"Loaded features: {features.shape[0]} APs x {features.shape[1]} features\")\n",
        "\n",
        "# 2) Standardize\n",
        "X = features.values.astype(float)\n",
        "X_scaled, mean_vec, std_vec = standardize(X)\n",
        "\n",
        "# 3) Baseline: multi-start k-means++ fixed at k=3\n",
        "baseline = multi_start_kmeans(X_scaled, k_range=[3], restarts=120, min_cluster_size=5, seed=99)\n",
        "baseline_summary = summarize_clusters(features, baseline['labels'])\n",
        "baseline_sil_values = silhouette_values(X_scaled, baseline['labels'])\n",
        "baseline_dbi = davies_bouldin_index(X_scaled, baseline['labels'])\n",
        "baseline_ch = calinski_harabasz_index(X_scaled, baseline['labels'])\n",
        "print(\n",
        "    f\"Baseline MS k-means++ (k=3): SSE={baseline['sse']:.2f}, \"\n",
        "    f\"silhouette={baseline['silhouette']:.3f}, sizes={baseline['sizes'].tolist()}\"\n",
        ")\n",
        "\n",
        "# 4) Optimizer: deterministic annealing fixed at k=3\n",
        "annealing = deterministic_annealing_cluster(X_scaled, k=3, T0=6.0, Tmin=0.01, alpha=0.9, inner_steps=12, seed=123)\n",
        "annealing_summary = summarize_clusters(features, annealing['labels'])\n",
        "anneal_sil_values = silhouette_values(X_scaled, annealing['labels'])\n",
        "anneal_dbi = davies_bouldin_index(X_scaled, annealing['labels'])\n",
        "anneal_ch = calinski_harabasz_index(X_scaled, annealing['labels'])\n",
        "print(\n",
        "    f\"Deterministic annealing (k=3): SSE={annealing['sse']:.2f}, \"\n",
        "    f\"silhouette={annealing['silhouette']:.3f}, sizes={annealing['sizes'].tolist()}\"\n",
        ")\n",
        "\n",
        "# 4b) Quick k-scan for both methods to show sensitivity\n",
        "k_grid = [2, 3, 4, 5]\n",
        "baseline_scan = []\n",
        "annealing_scan = []\n",
        "for k in k_grid:\n",
        "    if k == baseline['k']:\n",
        "        baseline_scan.append({'k': k, 'sse': float(baseline['sse']), 'silhouette': float(baseline['silhouette']), 'sizes': baseline['sizes'].tolist()})\n",
        "    else:\n",
        "        res = multi_start_kmeans(X_scaled, k_range=[k], restarts=80, min_cluster_size=5, seed=99)\n",
        "        baseline_scan.append({'k': k, 'sse': float(res['sse']), 'silhouette': float(res['silhouette']), 'sizes': res['sizes'].tolist()})\n",
        "    if k == annealing['k']:\n",
        "        annealing_scan.append({'k': k, 'sse': float(annealing['sse']), 'silhouette': float(annealing['silhouette']), 'sizes': annealing['sizes'].tolist()})\n",
        "    else:\n",
        "        res = deterministic_annealing_cluster(X_scaled, k=k, T0=6.0, Tmin=0.01, alpha=0.9, inner_steps=12, seed=123)\n",
        "        annealing_scan.append({'k': k, 'sse': float(res['sse']), 'silhouette': float(res['silhouette']), 'sizes': res['sizes'].tolist()})\n",
        "\n",
        "# 5) Package results\n",
        "results = {\n",
        "    'dataset': 'data 3 access point',\n",
        "    'n_samples': int(features.shape[0]),\n",
        "    'n_features': int(features.shape[1]),\n",
        "    'standardization': {'mean': mean_vec.tolist(), 'std': std_vec.tolist()},\n",
        "    'baseline': {\n",
        "        'method': 'multi-start k-means++ + Lloyd',\n",
        "        'k': int(baseline['k']),\n",
        "        'sse': float(baseline['sse']),\n",
        "        'silhouette': float(baseline['silhouette']),\n",
        "        'sizes': baseline['sizes'].tolist(),\n",
        "        'summary': baseline_summary,\n",
        "        'dbi': baseline_dbi,\n",
        "        'calinski_harabasz': baseline_ch,\n",
        "        'silhouette_summary': silhouette_summary(baseline_sil_values),\n",
        "    },\n",
        "    'annealing': {\n",
        "        'method': 'deterministic annealing for k-means SSE',\n",
        "        'k': int(annealing['k']),\n",
        "        'sse': float(annealing['sse']),\n",
        "        'silhouette': float(annealing['silhouette']),\n",
        "        'sizes': annealing['sizes'].tolist(),\n",
        "        'summary': annealing_summary,\n",
        "        'history': annealing['history'],\n",
        "        'dbi': anneal_dbi,\n",
        "        'calinski_harabasz': anneal_ch,\n",
        "        'silhouette_summary': silhouette_summary(anneal_sil_values),\n",
        "    },\n",
        "    'best_method': 'baseline' if baseline['silhouette'] >= annealing['silhouette'] else 'annealing',\n",
        "    'baseline_scan': baseline_scan,\n",
        "    'annealing_scan': annealing_scan,\n",
        "}\n",
        "\n",
        "# 6) Visualize clusters in 2D (PCA projection)\n",
        "coords, comps, mean_pca = pca_project(X_scaled, n_components=2)\n",
        "baseline_centroids_proj = project_points(baseline['centroids'], comps, mean_pca)\n",
        "annealing_centroids_proj = project_points(annealing['centroids'], comps, mean_pca)\n",
        "plot_clusters(coords, baseline['labels'], baseline_centroids_proj, 'Baseline k-means++ (PCA 2D)', 'baseline_clusters.png')\n",
        "plot_clusters(coords, annealing['labels'], annealing_centroids_proj, 'Deterministic Annealing (PCA 2D)', 'annealing_clusters.png')\n",
        "plot_silhouette_hist(baseline_sil_values, anneal_sil_values, 'silhouette_hist.png')\n",
        "plot_cooling_curve(annealing['history'], 'annealing_cooling.png')\n",
        "plot_k_scan(k_grid, baseline_scan, annealing_scan, 'k_scan_metrics.png')\n",
        "print(\n",
        "    'Saved visuals: baseline_clusters.png, annealing_clusters.png, silhouette_hist.png, '\n",
        "    'annealing_cooling.png, k_scan_metrics.png'\n",
        ")\n",
        "\n",
        "# 7) Save\n",
        "save_results(results)\n",
        "print('Wrote results_summary.json and results_summary.txt')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}